{
    "questions": [
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 1!",
            "question": "What the most efficient service can integrate data files from its on-premises with AWS Cloud via an NFS interface?",
            "answer": "Storage Gateway - File Gateway.",
            "srcImg": "",
            "descriptionP": "AWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud in order to store application data files and backup images as durable objects on Amazon S3 cloud storage. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching."
        },
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 2!",
            "question": "What feature of an Amazon S3 bucket only be suspended and not disabled once it have been enabled?",
            "answer": "Versioning.",
            "srcImg": "",
            "descriptionP": "Once you version-enable a bucket, it can never return to an unversioned state. Versioning can only be suspended once it has been enabled."
        },
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 3!",
            "question": "Massive volumes of data that can be divided into two categories. The 'hot data' need to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data need to be kept for reference with quick access for reads and updates at a low cost. What AWS services is BEST suited to accelerate the process?",
            "answer": "Amazon FSx for Lustre.",
            "srcImg": "",
            "descriptionP": "FSx for Lustre provides the ability to both process the 'hot data' in a parallel and distributed fashion as well as easily store the 'cold data' on Amazon S3."
        },
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 4!",
            "question": "A company utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service. What AWS service represent the best solution?",
            "answer": "Global Accelerator.",
            "srcImg": "",
            "descriptionP": "AWS Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover."
        },
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 5!",
            "question": "A company has a web application that runs 24*7 in the production environment. The company runs a clone of the same application in the dev environment for up to 8 hours every day. The company wants to build the MOST cost-optimal solution by deploying these applications using the best-fit pricing options for EC2 instances. What would you recommend?",
            "answer": "Use EC2 reserved instance for the production and on-demand instances for the dev.",
            "srcImg": "",
            "descriptionP": "For the given use case, you can use Amazon EC2 Reserved Instances for the prodution application as it is run 24*7; This way you can get a 72% discount if you avail a 3-year term. You can use on-demand instances for the dev application since it is only used for up to 8 hours per day. On-demand offers the flexibility to only pay for the Amazon EC2 instance when it is being used (0 to 8 hours for the given use case)."
        },
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 6!",
            "question": "A market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup to build a solution to fulfill this market need using Amazon API Gateway.",
            "answer": "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full duplex communication between client and server.",
            "srcImg": "",
            "descriptionP": "Amazon API Gateway creates RESTful APIs that: are HTTP-based. Enable stateless client-server communication. Implement standard HTTP methos such as GET, POST, PUT, PATCH, and DELETE. Also, Amazon API Gateway creates WebSocket APIs that: adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server. Route incoming messages based on message content. So Amazon API Gateway supports stateless RESTful APIs as well as stateful WebSocket APIs."
        },
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 7!",
            "question": "Amazon SQS is used for migrate several core applications to the cloud to ensure high availability and cost efficiency. The development team expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order. Which option can be used to implement this system?",
            "answer": "SQS FIFO (Fist-In-Fist-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate.",
            "srcImg": "",
            "descriptionP": "By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. Therefore you need to process 4 messages per operations so that the FIFO queue can support up to 1200 messages per second, which is well within the peak rate."
        },
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 8!",
            "question": "An EC2 instance 1A is running in AWS Region A. Later, it was created a new Amazon Machine Image (AMI) in Region A from a snapshot. This AMI is then copied into another Region B. It was provisioned an instance 1B in Region B using this new AMI in Region B. What entities exist in Region B?",
            "answer": "1 EC2 instance, 1 AMI and 1 snapshot exist in Region B.",
            "srcImg": "",
            "descriptionP": "An AMI provides the information required to launch an instance. You must specify an AMI when you launch an instance. When the new AMI is copied from Region A into Region B, it automatically creates a snapshot in Region B because AMIs are based on the underlying snapshots. Further, an instance is created from this AMI in Region B. Hence, we have 1 EC2, 1AMI and 1 snapshot in Region B."
        },
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 9!",
            "question": "A Fleet of EC2 instances realized a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset. Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of EC2 instances?",
            "answer": "Use Instance Store based EC2 instances.",
            "srcImg": "",
            "descriptionP": "An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host instance. Instance store is ideal for the temporary storage of information that changes frequently such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Instance store volumes are included as part of the instance's usage cost. As Instance Store based volumes provide high random I/O performance at low cost (as the storage is part of the instance's usage cost) and the resilient architecture can adjust for the loss of any instance, therefore you should use Instance Store based EC2 instances for this use-case."
        },
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 10!",
            "question": "Multiple microservices running on EC2 instances under an ALB. The team wants ti route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants request for https://www.test.com/orders to go to a specific microservice and request for https://www.test.com/products to go to another microservice. Which of the following features of ALB can be used?",
            "answer": "Path-based Routing.",
            "srcImg": "",
            "descriptionP": "Path-based Routing: you can route a client request based on the URL path of the HTTP header. You can use path conditions to define rules that route request based on the URL in the request (also known as path-based routing). The path pattern is applied only to the URL, not to its query parameters."
        },
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 11!",
            "question": "A dynamic website is hosted using on-premises servers in its data center in the USA. The company is lauching its website in ASIA, and it wants to optimize the website loading times for new users in ASIA. The website's backend must remain in the USA. The website is being launched in a few days, and an immediate solutions is needed. What would you recommend?",
            "answer": "Use CloudFront with a custom origin pointing to the on-premises servers.",
            "srcImg": "",
            "descriptionP": "CloudFront is a web service that gives businesses and web application developers an easy cost-effective way to distribute content with low latency and high data transfer speeds. CloudFront uses standard cache control headers you set on your files to identify static and dynamic content. You can use different origins for different types of content on a single site - e.g. S3 for static objects, EC2 for dynamic content, and custom origins for third-party content. An origin server stores the original, definitive version of your objects. If you're serving content over HTTP, your origin server is either an S3 bucket or an HTTP server, such as a web server. Your HTTP server can run on an EC2 instance or on a server that you manage, these servers are also known as custom origins."
        },
        {
            "title": "üèÜ Wellcome!!! Deep in the Content and Lost in the Knowledge - Resolution 12!",
            "question": "A company maintains the data for the last 100 years. The data has a velocity of 1 GB per minute. You would like to store the data with only the most relevant attributes. What AWS services would you use to build the most cost-effective solution with the LEAST amount of infrastructure maintenance?",
            "answer": "Ingest the data in Amazon Kinesis Data Firehose and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on S3.",
            "srcImg": "",
            "descriptionP": "Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you're already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. So, ingest the data in Amazon Kinesis Data Firehose and use a AWS Lambda function to filter and transform the incoming data before the output is dumped on S3. This way you only need to store a sliced version of the data with only the relevant data attributes required for your model. Also it should be noted that this solution is entirely serveless and requires no infrastructure maintenance."
        }

    ],
    "options": [
        {
            "option1": "a) Storage Gateway - Volume Gateway.",
            "option2": "b) Site-to-Site VPN.",
            "option3": "c) Storage Gateway - File Gateway.",
            "option4": "d) Storage Gateway - Tape Gateway.",
            "option5": "e) Instance Store."
        },
        {
            "option1": "a) Requester Pays.",
            "option2": "b) Server Access Logging.",
            "option3": "c) Versioning.",
            "option4": "d) Policies.",
            "option5": "e) Static Website Hosting."
        },
        {
            "option1": "a) EMR.",
            "option2": "b) Glue.",
            "option3": "c) S3 Glacier Deep Archive.",
            "option4": "d) Amazon FSx for Windows File Server.",
            "option5": "e) Amazon FSx for Lustre."
        },
        {
            "option1": "a) CloudFront.",
            "option2": "b) Third-Party services.",
            "option3": "c) Route 53.",
            "option4": "d) Global Accelerator.",
            "option5": "e) ELB."
        },
        {
            "option1": "a) Use EC2 reserved instance for the production and spot instances for the dev.",
            "option2": "b) Use EC2 reserved instance for the production and spot block intances for the dev.",
            "option3": "c) Use EC2 reserved instance for the production and on-demand instances for the dev.",
            "option4": "d) Use only EC2 on-demand instaces for the production and dev.",
            "option5": "e) Use on-demand EC2 instances for the production and spot intances for the dev."
        },
        {
            "option1": "a) Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full duplex communication between client and server.",
            "option2": "b) Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full duplex communication between client and server.",
            "option3": "c) Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full duplex communication between client and server.",
            "option4": "d) Amazon API Gateway creates RESTful APIs that enable both stateless and stateful client-server communication.",
            "option5": "e) Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full duplex communication between client and server."
        },
        {
            "option1": "a) Use SQS standard queue to process the messages.",
            "option2": "b) SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate.",
            "option3": "c) SQS FIFO (First-In-First-Out) queue to process the messages.",
            "option4": "d) SQS FIFO (Fist-In-Fist-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate.",
            "option5": "e) Use both SQS standard queue and SQS FIFO queue to process the messages."
        },
        {
            "option1": "a) 1 EC2 instance and 1 snapshot exist in Region B.",
            "option2": "b) 1 EC2 instance and 1 AMI exist in Region B.",
            "option3": "c) 1 EC2 instance, 1 AMI and 2 snapshots.",
            "option4": "d) 1 EC2 instance and 2 AMIs exist in Region B.",
            "option5": "e) 1 EC2 instance, 1 AMI and 1 snapshot exist in Region B."
        },
        {
            "option1": "a) Use EBS based EC2 instances.",
            "option2": "b) Use EC2 instances with access to S3 based storage.",
            "option3": "c) Use EC2 instances with EFS mount points.",
            "option4": "d) Use EC2 instances with EBS and EFS mount points.",
            "option5": "e) Use Instance Store based EC2 instances."
        },
        {
            "option1": "a) Host-based Routing.",
            "option2": "b) Query string parameter-based routing.",
            "option3": "c) Path-based Routing.",
            "option4": "d) HTTP method-based routing.",
            "option5": "e) HTTP header-based routing."
        },
        {
            "option1": "a) Leverage a Route 53 geo-proximity routing policy pointing to on-premises servers.",
            "option2": "b) Migrate the website to S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the USA and ASIA.",
            "option3": "c) Use CloudFront with a custom origin pointing to the DNS record of the website on Route 53.",
            "option4": "d) Use CloudFront with a custom origin pointing to the on-premises servers.",
            "option5": "e) Migrate the dynamic website to EC2."
        },
        {
            "option1": "a) Ingest the data in Amazon Kinesis Data Firehose and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on S3.",
            "option2": "b) Ingest the data in a Spark Streaming Cluster on EMR and use Spark Streaming transformations before writing to S3.",
            "option3": "c) Ingest the data in Amazon Kinesis Data Streams and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on S3.",
            "option4": "d) Ingest the data in Amazon Kinesis Data Analytics and use Amazon Kinesis Data Firehose to filter and transform the data before the output is dumped on S3.",
            "option5": "e) Ingest the data in Amazon Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to S3."
        }
    ]
}
